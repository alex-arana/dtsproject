#labels DMI,JSDL
= DMI Discussion Points / Possibilities/ Proposals =

== Intro ==
We are implementing a scalable data transfer service that performs recursive directory copying between different (incompatible) storage/file systems, including, GridFTP, iRODS, SRB, FTP, SFTP, HTTP(S), local FILE, WEBDAV?. 
The service is based on an architecture that deploys data transfer worker agents implemented using Apache Virtual File System (VFS) into a fully scalable worker node pool. Remote worker nodes are invoked via asynchronous message queues, which are implemented using JMS (i.e. asynchronous point-to-point message channels). Worker agents can be installed within a particular network topology, the only requirement is that a worker can access the (remote) trusted message broker. In doing this, workers can be strategically deployed at or close to a particular data source and/or sink (i.e. facilitating both access to the data and for improving transfer efficacy).   

== Proposed Message Format / Messaging Model ==
To implement this service, we require a message format for describing a data transfer that may consist of multiple data sources and sinks. At present, we have proposed two potential composite schemas, one extends DMI^1^, the other modifies JSDL HPC file staging profile^2^ . Ideally, we would like to produce a single standards compliant message format. 
This wiki page identifies some issues/discussion points and proposals.  

^1^OGSA Data Movement Interface: http://www.ogf.org/documents/GFD.134.pdf
<br/>
^2^JSDL HPC File Staging Profile: http://www.ogf.org/documents/GFD.135.pdf


Two key requirements of the message format are:
  # All messages must be fully self contained and place no assumptions on the underlying transport mechanism so that they are fully transport agnostic. The messaging model should be applicable for use with the following delivery methods: 
    * used within the soap bodies of doc-literal Web Service invocations/responses
    * posted to a RESTfull endpoint
    * posted to/from an asynchronous message channel (e.g. JMS queue/topic)
    * email
  # We need to define multiple data transfers in the same request message. 




=Proposals=
To do this, we devised the following proposals: 
  * *1) <a href="#dmib">DMIB Proposal (DMI for _Bulk_-transfers)</a>*
<br/>
A 'dmi-wrapped' rendering proposal that effectively combines a source and a sink DEPR within a single element which can be defined multiple times in a single request/packet. This is very similar to the {{{<jsdl:DataStaging>}}} elements that can be defined multiple times within a single JSDL document.


  * *2) A modified version of the JSDL file staging profile*
<br/>
This proposal adds some extra information (inc. the MinxTransferRequirementsType which itself extends the DMI TransferRequirementsType, a re-positioned Credential element within the src and target URI element, and an abstract URIProperties element that can be used to define additional information for connecting to the the src/target URI). 

<br/>
<hr/>
<br/>

=1) DMIB Proposal Schema= 
<a name="dmib">(DMIB for Bulk-transfers)</a> Note, this proposal does not require modification to the existing DMI spec (other than the addition of some {{{<xsd:any/>}}} extension points), rather, it builds on the DMI spec by re-using the existing elements within a new 'wrapped' schema.
<br/>
  * Proposal schema: http://code.google.com/p/dtsproject/source/browse/trunk/dts-jaxb/src/main/resources/archive/dmi-wrappedMessagesProposals.xsd 

  * Doc example: http://code.google.com/p/dtsproject/source/browse/trunk/dts-jaxb/src/main/resources/archive/dmi-WrappedDataTransferRequest.xml
<br/>
<br/>

==DMIB Proposal features==

===Use of JobID to enable Doc-literal/RESTfull renderings ===
We require a document-literal type approach that does not use WSRF service factories and instances that are accessed via WS-Addressing Endpoint References. 

The key difference between this document-centric rendering (above) and the current DMI WSRF approach, is that the doc-centric approach *passes a JobID back to the client rather than a WS-Addressing Endpoint Reference* (which point to a DMI service Instance previously created from a factory). In addition, since there is no separate service instance, the client must submit the JobID to the service on each subsequent status request/query. 

===Accommodation of multiple transfers === 
The current DMI functional spec is used to define a single data transfer between a single source and a single sink. A single data aggregate, such as a directory that resides at one sink can be specified in the [data EPR] (e.g. by appending the '/' char to an EPR to identify a directory). However this appears to account for only one directory. We have a requirement to be able to define multiple files and/or directories, potentially residing at different locations. 
In order to accommodate multiple transfers in DMI, the client interacts with a DMI factory to create multiple service instances, each responsible for a separate transfer (please correct me/comment on this if i am wrong!). Potentially, this can place a large communication overhead on the client since many interactions are required between the client and service (e.g. consider the case when copying many ~1000 different files from a number of different sources). 
 
We therefore need to be able to define multiple data transfers within the same request message/packet so that the client can be very thin, i.e. fire a single request and periodically poll for status updates (see http://www.eaipatterns.com/MessagingComponentsIntro.html for the 'atomic' message/packet definition). 

To do this, we have devised the following 'dmi-wrapped' rendering proposal that effectively combines a source and a sink DEPR within a single element which can be defined multiple times in a single request/packet. This is very similar to the {{{<jsdl:DataStaging>}}} elements that can be defined multiple times within a single JSDL document. See pseudo code-frag below;

{{{
   Pseudo schema request:
    ======================
    <dmi-msg:SubmitWrappedDataTransferRequestMessage>
        <dmi-msg:Start/>
        <dmi-msg:WrappedSourceSinkDEPRs>
            <dmi-msg:SourceDEPR></dmi-msg:SourceDEPR>
            <dmi-msg:SinkDEPR></dmi-msg:SinkDEPR>
            <dmi-msg:TransferRequirements/>
        </dmi-msg:WrappedSourceSinkDEPRs>
        <dmi-msg:WrappedSourceSinkDEPRs>
            <dmi-msg:SourceDEPR></dmi-msg:SourceDEPR>
            <dmi-msg:SinkDEPR></dmi-msg:SinkDEPR>
            <dmi-msg:TransferRequirements/>
        </dmi-msg:WrappedSourceSinkDEPRs>
        <dmi-msg:WrappedSourceSinkDEPRs>
            <dmi-msg:SourceDEPR></dmi-msg:SourceDEPR>
            <dmi-msg:SinkDEPR></dmi-msg:SinkDEPR>
            <dmi-msg:TransferRequirements/>
        </dmi-msg:WrappedSourceSinkDEPRs>
        ...
    </dmi-msg:SubmitWrappedDataTransferRequestMessage>

    Pseudo schema response:
    =======================
   <dmi-msg:GetWrappedDataTransferInstanceResponseMessage>
       <dmi-msg:JobID>jobid-adfafq24-59-4-13</dmi-msg:JobID>
   </dmi-msg:GetWrappedDataTransferInstanceResponseMessage>
}}}

===JobID for bulk transfer and for sub-transfers===
*Single Bulk JobID for whole transfer*
At present the proposal above returns a single JobID for the whole 'bulk' transfer (since a bulk transfer can be made up of multiple sub transfers). This implies that any subsequent request that uses this JobID (e.g. a request for state, or a request for the dmi instance attributes document) refers to the state of the whole 'bulk' transfer. For example, the total number of bytes transferred would be calculated across all the sub-transfers. Similarly, if one sub-transfer failed out of ten, then the state of the bulk transfer would be 'failed'. Note, this has similar semantics to defining an aggregate [data EPR] that represents a data aggregate residing on the same source, such as a single directory (see current DMI spec). 

*BulkJobID + subtransferAliasID for sub-transfer status*
This could certainly be extended further if it is a (probable) requirement to drill down and query the state of each separate sub-transfer. To do this, the request message (GetDataTransferInstanceRequestType) could be extended to incorporate an additional user defined sub-transfer tag/id/alias (i.e. a unique element or attribute) which could be defined for each separate sub-transfer, such as mytransferA, mytransferB, mytransferC. In doing this, any subsequent request made by the client could use a combination of the JobID and the sub-transfer tag/id {{{(ie. '<JobID>:<transferTag>')}}} to isolate that specific sub-transfer. The returned state would then represent only that sub-transfer as per the dmi spec.

*Note*, in this scenario, it would be important that the client assigns the sub-transfer tags/ids/alias while the service assigns the bulk transfer JobID. This is because:
  * We do not want to place any significance on the ordering of the sub-transfers in the initial request.
  * Client can assign a meaningful name to the sub-transfer (e.g. 'BulkJobID:myCriticalFile'). 
  

*Note*, this would be a requirement if the multi-transfer request is split into separate message packets by the service and each sub-transfer packet is processed separately, rather than sending the whole composite data transfer request to the JMS queue to be processed by a single worker. See message splitter and aggregator patterns (http://www.eaipatterns.com/Sequencer.html, http://www.eaipatterns.com/Aggregator.html).


===Addition of JSDL CreationOption in DMI ===
Add the the {{{<jsdl:CreationFlag>}}} element, which defines overwrite, dontOverwrite and append enums, to the {{{<dmi:TransferRequirementsType>}}}, e.g. something like the following, but define the CreationFlag within the dmi namespace ?

{{{
        <dmi-msg:TransferRequirements>
            <!--<dmi:StartNotBefore></dmi:StartNotBefore>
            <dmi:EndNoLaterThan>6</dmi:EndNoLaterThan>
            <dmi:StayAliveTime>6</dmi:StayAliveTime>-->
            <dmi:MaxAttempts>2</dmi:MaxAttempts>
            <!-- define CreationFlag in dmi ns ? --> 
            <jsdl:CreationFlag>overwrite</jsdl:CreationFlag>
        </dmi-msg:TransferRequirements>

}}} 



=== Extension points in DMI schema ===
A number of xsd extension points (xsd:any) are required in the current dmi functional spec, e.g in the following DMI elements
  * {{{<dmi:Data/>}}} element 
  * list elements here - is it a good idea to add {{{<xsd:any/>}}} to all dmi complex types much like JSDL ? 

<br/>
<hr/>
<br/>

=2) Modified and extended JSDL HPC File Staging Profile=
<a name="modjsdlfilestage"/>
The JSDL file staging profile could potentially be used to implement a data transfer style service, but this spec is focused mostly on compute. We have used {{{<xsd:redef/>}}} to extend and modify the profile for bulk data transfers. 
<br/>
<br/>
  * Proposal schema: http://code.google.com/p/dtsproject/source/browse/trunk/dts-schema/src/main/resources/minx-jsdl.xsd

  * Doc example: http://code.google.com/p/dtsproject/source/browse/trunk/dts-schema/src/main/resources/minx-jsdl-example.xml
<br/>

=== Vanilla HPC file staging profile ===

The HPC file staging profile supports a credential nested within the {{{<DataStaging>}}} element. However, this supports only one credential, either for the {{{<Source>}}} or {{{<Target>}}}, not both. This is by design, *it serves the purpose of staging files to an intermediary (i.e. the compute resource) and then optionally staging files from the intermediary following job completion. The source and sink can be linked by defining two DataStaging elements with the same {{{<jsdl:FileName/>}}} element as below*. 

{{{
<DataStaging>
    <FileName>fileA.txt</FileName>
    <CreationFlag>overwrite</CreationFlag>
    <Source>
       <URI>ftp://server1.inthe.sky:1234</URI>
    </Source>
    <Credential xmlns="http://schemas.ogf.org/hpcp/2007/11/ac">
      <UsernameToken xmlns="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd">
        <Username>hi</Username>
        <Password>world</Password>
      </UsernameToken>
    </Credential>
</DataStaging>


<DataStaging>
    <FileName>fileA.txt</FileName>
    <CreationFlag>overwrite</CreationFlag>
    <Target>
       <URI>ftp://server2.inthe.sky:1234</URI>
    </Target>
    <Credential xmlns="http://schemas.ogf.org/hpcp/2007/11/ac">
      <UsernameToken xmlns="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd">
        <Username>demo</Username>
        <Password>pass</Password>
      </UsernameToken>
    </Credential>
</DataStaging>
}}}

==Modified HPC File Staging Proposal features==
While the vanilla hpc file staging profile serves the purpose of staging data to and from a compute resource, *the notion of the intermediary is redundant in a DMI type service* (i.e. to link the source and target destinations on the intermediary using {{{<jsdl:FileName/> and <jsdl:FileSystem/>}}} elements is unnecessary overhead). To get round this issue, it is possible to redfine the jsdl hpc schema slightly (using {{{<xsd:redef/>}}}) to override and modify the existing jsdl schema to: 
  # redefine {{{<jsdl:DataStaging>}}} as {{{<jsdl:DataTransfer>}}}
  # allow the nesting of credentials in the source and target elements (as below)
  # add an abstract URIProperties element required to define additional necessary information for the service to contact the data source and sink, e.g. srb requires McatZone, MdasDomain information. The URIProperties element is abstract so that it can be implemented for different protocols using a substitution group. So far, we have defined GridFTPURIProperties and SrbProtocolProperties used to define additional information (e.g. PortRange, MdasDomain, McatZone etc). 



*Modified JSDL file staging example:*
{{{
   <mjsdl:DataTransfer>
      <mjsdl:Source>
        <jsdl:URI>srb://ng2.vpac.org/etc/termcap</jsdl:URI>
        <mjsdl:Credential>
            <mjsdl:MyProxyToken>
                <mjsdl:MyProxyUsername>${myproxy.username}</mjsdl:MyProxyUsername>
                <mjsdl:MyProxyPassword>${myproxy.password}</mjsdl:MyProxyPassword>
                <mjsdl:MyProxyServer>myproxy2.arcs.org.au</mjsdl:MyProxyServer>
                <mjsdl:MyProxyPort>7512</mjsdl:MyProxyPort>
            </mjsdl:MyProxyToken>
             <!--
             define protocol specifics for data access 
             -->
            <mjsdl:SrbProtocolProperties>
                <mjsdl:McatZone>mymcatZone</mjsdl:mjsdl:McatZone>
                <mjsdl:MdasDomain>my mdas domain </mjsdl:MdasDomain>
                <!--<mjsdl:HomeDirectory>/home/whatever</mjsdl:HomeDirectory>
                <mjsdl:DefaultResource>ng2.vpac.org</mjsdl:DefaultResource>-->
                <mjsdl:PortRange>
                    <mjsdl:portMin>6400</mjsdl:portMin>
                    <mjsdl:portMax>6500</mjsdl:portMax>
                </mjsdl:PortRange>
             </SrbProtocolProperties>
        </mjsdl:Credential>
      </mjsdl:Source>

      <mjsdl:Target>
        <jsdl:URI>ftp://dm11.intersect.org.au/upload/pom-from-pub.xml</jsdl:URI>
        <mjsdl:Credential>
            <wsse:UsernameToken>
              <wsse:Username>${ftp.username}</wsse:Username>
              <wsse:PasswordString>${ftp.password}</wsse:PasswordString>
            </wsse:UsernameToken>
        </mjsdl:Credential>
      </mjsdl:Target>
      <mjsdl:TransferRequirements>
        <dmi:MaxAttempts>0</dmi:MaxAttempts>
        <jsdl:CreationFlag>overwrite</jsdl:CreationFlag>
      </mjsdl:TransferRequirements>
    </mjsdl:DataTransfer>

}}}



Please take a look at for more details: <br/>
http://code.google.com/p/dtsproject/source/browse/trunk/dts-schema/src/main/resources 
  