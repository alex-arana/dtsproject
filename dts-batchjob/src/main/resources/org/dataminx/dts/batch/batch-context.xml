<?xml version="1.0" encoding="UTF-8"?>

<!--
This is the Spring configuration file for the DTS Spring Batch-job (dtsFileTransferJob).
-->

<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:batch="http://www.springframework.org/schema/batch"
    xmlns:aop="http://www.springframework.org/schema/aop"
    xmlns:tx="http://www.springframework.org/schema/tx"
    xmlns:p="http://www.springframework.org/schema/p"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:util="http://www.springframework.org/schema/util"
    xmlns:context="http://www.springframework.org/schema/context"
    xsi:schemaLocation="
        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-2.5.xsd
        http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-2.0.xsd
        http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-2.0.xsd
        http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-2.0.xsd
        http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-2.0.xsd
        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd">

  <!--
  Import the spring config files in dataminx.dir, inc:
  - batch-job-db-context.xml (for the 'dataSource' bean)
  - plugin-context.xml (for the 'encryptionPlugin' bean)
  -->
  <import resource="file:${dataminx.dir}/*-context.xml"/>  



  <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager" lazy-init="true">
    <property name="dataSource" ref="dataSource" />
  </bean>

  <bean id="dtsVfsUtil" class="org.dataminx.dts.common.vfs.DtsVfsUtil">
    <property name="ftpSupported" value="true" />
    <property name="sftpSupported" value="true" />
    <property name="httpSupported" value="true" />
    <property name="gsiftpSupported" value="true" />
    <property name="srbSupported" value="true" />
    <property name="fileSupported" value="true" />
    <property name="irodsSupported" value="true" />
    <property name="myProxyCredentialLifetime" value="${default.myproxy.lifetime}" />
    <property name="credentialStore" ref="credentialStore"/>
  </bean>
  
  <bean id="jobExplorer" class="org.springframework.batch.core.explore.support.JobExplorerFactoryBean">
    <property name="dataSource" ref="dataSource"/>
  </bean>

  <!--
  The jobRepository is used to persist various Spring-Batch domain objects, such
  as the JobExecution and StepExecution contexts. The 'jobRepository' id means that this
  repo is auto-referenced by its collaborators (such as the job) who do no have to
  explicitly reference the repo.
  -->
  <bean id="jobRepository"
        class="org.springframework.batch.core.repository.support.JobRepositoryFactoryBean">
    <property name="dataSource" ref="dataSource"/>
    <property name="transactionManager" ref="transactionManager"/>
  </bean>
  
  <bean id="dtsJobFactory" class="org.dataminx.dts.batch.DtsJobFactoryImpl">
    <property name="jobRepository" ref="jobRepository"/>
    <property name="credentialStore" ref="credentialStore"/>
  </bean>

  <!-- the task executor below will have to be provided by the calling application  -->
  <bean id="dtsJobLauncher" class="org.dataminx.dts.batch.DtsJobLauncher">
    <property name="jobRepository" ref="jobRepository" />
    <property name="taskExecutor" ref="jobLauncherTaskExecutor"/>
    <property name="dtsJobFactory" ref="dtsJobFactory"/>
    <property name="dtsJobDefinitionValidator" ref="dtsJobDefinitionValidator"/>
    <property name="messageSource" ref="messageSource" />
  </bean>


  <!--
  Step: JobScopingTask implements Tasklet (a Step with no ItemReader, ItemWriter).
  -->
  <bean id="jobScopingTask" class="org.dataminx.dts.batch.JobScopingTask" scope="step">
    <property name="submitJobRequest" value="#{jobExecutionContext[SUBMIT_JOB_REQUEST]}"/>
    <property name="jobPartitioningStrategy" ref="jobPartitioningStrategy" />
    <!-- job_resource_key == jobId == messageID == correlationID == same value -->
    <property name="jobResourceKey" value="#{jobExecutionContext[JOB_RESOURCE_KEY]}"/>
    <property name="jobTag" value="#{jobExecutionContext[JOB_TAG]}"/>
    <property name="jobNotificationService" ref="jobNotificationService"/>
    <property name="executionContextCleaner" ref="executionContextCleaner"/>
  </bean>


  <!--
  Step: CheckRequirementsTask implements Tasklet
  -->
  <bean id="checkRequirementsTask" class="org.dataminx.dts.batch.CheckRequirementsTask" scope="step">
    <!--<property name="jobStepDirectory" value="file://${user.home}/.dataminx/jobsteps" />-->
    <property name="jobStepDirectory" value="file:${dataminx.dir}/jobsteps" />
    <!--<property name="jobStepDirectory" value="file:/home/djm76/.dataminx/jobsteps"/>-->
  </bean>

  <!--
  ExecutionContextPromotionListener used to automatically promote items
  from the Step ExecutionContext to the Job ExecutionContext at the
  end of a step. A list of keys should be provided that correspond to the items
  in the Step ExecutionContext that should be promoted.
  To define multiple keys use: <beans:property name="keys" value="prop1,prop2,prop3">
  -->
  <bean id="jobScopingTaskPromotionListener" class="org.springframework.batch.core.listener.ExecutionContextPromotionListener">
    <property name="keys" value="JOB_DETAILS" />
  </bean>
  
  <bean id="fileCopyTaskPromotionListener" class="org.springframework.batch.core.listener.ExecutionContextPromotionListener">
    <property name="keys" value="lastCompletedSuspendedStep" />
  </bean>

  <!--
  Step: FileCopyTask implements Tasklet
  This step uses late-binding so it must have step scope. This step is a PARTITIONED
  step (and so there can be many instances of this particular step created at runtime).
  Each instance is responsible for processing a single step file that records
   a list of DTUs (i.e. a list of file-only source-to-sink transfers).
  The step's jobStep value will be set by late-binding at runtime from the stepExeCtxt.
  The steps submitJobRequest will be set by late-binding at runtime from the jobExeCtxt.
  The step will be issued with a particular step file
  -->
  <bean id="fileCopyTask" class="org.dataminx.dts.batch.FileCopyTask" scope="step">
    <property name="submitJobRequest" value="#{jobExecutionContext[SUBMIT_JOB_REQUEST]}"/>
    <property name="jobStep" value="#{stepExecutionContext[DATA_TRANSFER_STEP]}"/>
    <property name="dtsVfsUtil" ref="dtsVfsUtil"/>
    <property name="jobNotificationService" ref="jobNotificationService"/>
    <property name="fileCopyingService" ref="fileCopyingService"/>
    <property name="executionContextCleaner" ref="executionContextCleaner"/>
    <property name="fileSystemManagerCache" ref="fileSystemManagerCache"/>
    <property name="stopwatchTimer" ref="stopwatchTimer"/>
    <property name="encrypter" ref="encryptionPlugin" />
    <property name="jobOperator" ref="jobOperator" />
  </bean>
  
  <bean id="fileCopyingService" class="org.dataminx.dts.batch.service.FileCopyingServiceImpl">
    <property name="dtsVfsUtil" ref="dtsVfsUtil"/>
    <property name="encrypter" ref="encryptionPlugin" />
  </bean>


  <!--
  Job: dtsFileTransferJob extends SimpleJob
  A spring job is a container for steps. A job has 3 required dependencies:
  A a) name, b) JobRepository and c) a list of Steps. The repo does not have to
  be explictly referenced using a 'job-repository' attribute because it has the
  default name of 'jobRepository'.
  This job allows for restarts. It is considered a 'restart' if a JobExecution
  already exists for the particular JobInstance in the repo.
  -->
  <bean id="dtsFileTransferJob" scope="prototype" class="org.dataminx.dts.batch.DtsFileTransferJob">
    <property name="checkRequirementsStep" ref="checkRequirementsStep"/>
    <property name="jobScopingStep" ref="jobScopingStep"/>
    <property name="maxStreamCountingStep" ref="maxStreamCountingStep"/>
    <property name="partitioningStep" ref="partitioningStep"/>
    <property name="jobExecutionListeners">
      <list>
        <ref bean="dtsFileTransferJobListener"/>
      </list>
    </property>
    <property name="stopwatchTimer" ref="stopwatchTimer"/>
    <property name="jobPartitioningStrategy" ref="jobPartitioningStrategy"/>
  </bean>
  
  <bean id="dtsJobDefinitionValidator" class="org.dataminx.dts.common.validator.DefaultDtsJobDefinitionValidator">
    <!--<property name="jobIdentificationValidator" ref="jobIdentificationValidator"/>
    <property name="sourceTargetValidator" ref="sourceTargetValidator"/>-->
    <property name="dataLocationsTypeValidator" ref="dataLocationsTypeValidator"/>
  </bean>

  <bean id="dataLocationsTypeValidator" class="org.dataminx.dts.common.validator.DefaultDataLocationsValidator"/>
  <!--
  <bean id="jobIdentificationValidator" class="org.dataminx.dts.common.validator.DefaultJobIdentificationValidator"/>
  <bean id="sourceTargetValidator" class="org.dataminx.dts.common.validator.DefaultSourceTargetValidator"/>
  -->

  <bean id="messageSource" class="org.springframework.context.support.ResourceBundleMessageSource">
    <property name="basename" value="validation-errors"/>
  </bean>

  <bean id="stopwatchTimer" class="org.dataminx.dts.common.util.StopwatchTimer"/>
  
  <bean id="dtsFileTransferJobListener" class="org.dataminx.dts.batch.DtsFileTransferJobListener">
    <property name="fileSystemManagerCache" ref="fileSystemManagerCache"/>
    <property name="executionContextCleaner" ref="executionContextCleaner"/>
    <property name="jobExplorer" ref="jobExplorer"/>
    <property name="dtsVfsUtil" ref="dtsVfsUtil"/>
  </bean>
    
  <batch:step id="jobScopingStep">
    <batch:tasklet ref="jobScopingTask">
      <batch:listeners>
          <batch:listener ref="jobScopingTaskPromotionListener"/>
      </batch:listeners>    
    </batch:tasklet>
  </batch:step>
  
  <batch:step id="checkRequirementsStep">
    <batch:tasklet ref="checkRequirementsTask" allow-start-if-complete="true"/>
  </batch:step>

  <batch:step id="fileCopyStep">
    <batch:tasklet ref="fileCopyTask">
      <batch:listeners>
          <batch:listener ref="fileCopyTaskPromotionListener"/>
      </batch:listeners>    
    </batch:tasklet>
  </batch:step>


  <!--
  The Master PartionStep for partitioning the fileCopyStep
  i.e. create muliple fileCopyStep instances and execute them all
  using a synchronous task-execution grid (gridSize=1).
  The 'PartitionStep' drives execution of the fileCopySteps using the
  required partitionHandler and stepExecutionSplitter interfaces which must
  be implemented for the specific environment.

  The number of fileCopyStep instances is derived from the jobScoping task which
  generates a list of DtsJobStep instances (with corresponding files) which get
  injected into the file fileCopyStep at runtime using late-binding, thus:
  numb of DtsJobSteps == numb of fileCopyStep instances.

  -->
  <bean id="partitioningStep"
        class="org.springframework.batch.core.partition.support.PartitionStep">
    <!--
    The PartitionHandler sends StepExecution requests to the (potentially remote) partitioned steps.
    The partitionHandler therefore must know about the fabric of the remoting/grid environment.
    Spring Batch provides a useful implementation of PartitionHandler that executes Steps locally in
    separate threads of execution using Spring's TaskExecutor strategy.
    -->
    <property name="partitionHandler">
      <bean class="org.springframework.batch.core.partition.support.TaskExecutorPartitionHandler">
        <property name="taskExecutor">
          <bean class="org.springframework.core.task.SyncTaskExecutor"/>
        </property>
        <!-- 
        The fileCopySteps are run in a parallel exeuction grid (But with gridSize set to one,
        which is the default, we simply run fileCopyStep partitions/instances sequentially).
        -->
        <property name="step" ref="fileCopyStep"/>
        <!-- Rely on the default gridSize of one 
        (Used to specify the number of separate step executions to create/run in parallel
        (i.e. a parallel partition) so it can be matched to the size of the
        thread pool in the TaskExecutor)-->
        <!--<property name="gridSize" value="1" />-->
      </bean>
    </property>

    <!--
    The partitioner's role (required by the stepExecutionSplitter) is to generate
    a new step ExecutionContext (as an input parameter) for each partitioned step instance.
    Use a generic implementation of StepExecutionSplitter that delegates to a
    Partitioner to generate the required number of Step ExecutionContext instances.-->
    <property name="stepExecutionSplitter">
      <bean class="org.springframework.batch.core.partition.support.SimpleStepExecutionSplitter">
        <constructor-arg ref="jobRepository"/>
        <constructor-arg ref="fileCopyStep"/>
        <constructor-arg ref="dtsJobPartitioner"/>
      </bean>
    </property>
    <property name="jobRepository" ref="jobRepository"/>
  </bean>


  <bean id="dtsJobPartitioner" scope="step" class="org.dataminx.dts.batch.DtsJobPartitioner">
    <property name="dtsJobDetails" value="#{jobExecutionContext[JOB_DETAILS]}"/>
  </bean>
  
  
  <!-- DIFFERENT JOB STEP ALLOCATOR TO BE USED BY THE JOB PARTITIONING STRATEGY -->
  <!-- A job step allocator that mixes big and smalls together in a step -->
  <bean id="mixedFilesJobStepAllocator" class="org.dataminx.dts.batch.MixedFilesJobStepAllocator" scope="prototype"/>
  <!-- A job step allocator that groups big files together (and same with small files) in a step -->
  <bean id="categorizedFilesJobStepAllocator" class="org.dataminx.dts.batch.CategorizedFilesJobStepAllocator" scope="prototype">
    <property name="bigFileSize" value="${min.bigfiles.bytes.size}"/>
  </bean>
  
  
  <bean id="jobPartitioningStrategy" class="org.dataminx.dts.batch.AbstractJobPartitioningStrategy">
    <!-- lookup-method injection of createDtsJobStepAllocator method.
         replace the jobStepAllocator implementation here if you want to use a
         new strategy for partitioning files -->
    <lookup-method  name="createDtsJobStepAllocator" bean="mixedFilesJobStepAllocator"/>
    <property name="dtsVfsUtil" ref="dtsVfsUtil"/>
    <property name="totalFilesLimit" value="${total.transfer.files.limit}"/>
    <property name="totalSizeLimit" value="${total.transfer.size.limit}"/>
    <property name="maxTotalByteSizePerStepLimit" value="${max.total.bytes.perstep}" />
    <property name="maxTotalFileNumPerStepLimit" value="${max.total.num.files.perstep}" />
    <property name="encrypter" ref="encryptionPlugin" />
  </bean>

  <!--
  In the case of a restartable job, there may be steps that should always
  be run, regardless of wheter or not they were successful the first time. During
  NORMAL processing of a restarted job, any step with a status of 'COMPLETED' will
  be skipped. Setting allow-start-if-complete to true overrides this so the
  step will always run:
  -->
  <batch:step id="maxStreamCountingStep">
    <batch:tasklet ref="maxStreamCounterTask" allow-start-if-complete="true"/>
  </batch:step>
  
  <bean id="maxStreamCounterTask" class="org.dataminx.dts.batch.MaxStreamCounterTask" scope="step">
    <property name="submitJobRequest" value="#{jobExecutionContext[SUBMIT_JOB_REQUEST]}"/>
    <property name="maxConnectionsToTry" value="${default.max.parallel.connections}"/>
    <property name="dtsVfsUtil" ref="dtsVfsUtil"/>
    <property name="fileSystemManagerCache" ref="fileSystemManagerCache" />
    <property name="jobRepository" ref="jobRepository"/>
    <property name="dtsJobDetails" value="#{jobExecutionContext[JOB_DETAILS]}"/>
    <property name="encrypter" ref="encryptionPlugin" />
  </bean>


  <!--
  Stores a list of FileSystemManagers under a RootURLString.
  -->
  <bean id="fileSystemManagerCache" class="org.dataminx.dts.common.vfs.FileSystemManagerCache"/>
  
  <bean id="executionContextCleaner" class="org.dataminx.dts.batch.common.util.ExecutionContextCleaner">
    <property name="jobRepository" ref="jobRepository"/>
    <property name="jobExplorer" ref="jobExplorer"/>
  </bean>
  
  <bean id="jobOperator" class="org.springframework.batch.core.launch.support.SimpleJobOperator">
    <property name="jobExplorer" >
        <bean class="org.springframework.batch.core.explore.support.JobExplorerFactoryBean">
            <property name="dataSource" ref="dataSource" />
        </bean>
    </property>
    <property name="jobRepository" ref="jobRepository" />
    <property name="jobRegistry" ref="jobRegistry" />
    <property name="jobLauncher" ref="dtsJobLauncher" />
  </bean>
  
  <bean id="jobRegistry" class="org.springframework.batch.core.configuration.support.MapJobRegistry" />
  <bean id="jobRegistryBeanPostProcessor" class="org.springframework.batch.core.configuration.support.JobRegistryBeanPostProcessor">
    <property name="jobRegistry" ref="jobRegistry"/>
  </bean>


<!--
    	<bean id="encryptionPlugin" class="org.dataminx.dts.security.crypto.DummyEncrypter">
		<property name="passphrase" value="dummy"/>
		<property name="salt" value="dummy"/>
	</bean>
    -->
  
</beans>
